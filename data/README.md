The following data is the basis for training. Only german data is included. Contributors dissociate from all expressed opinions.

## Data

The data preparation takes place in three steps:
1. [Download data](#raw) where data from available ressources is downloaded
2. [Preprocessing](#preprocess) where each data source is converted to a uniform format, i.e. columns text and label whereby label 0 represents non-vulgar text and label 1 represents vulgar text.
3. [Training](#train) where all individual sources are merged into one large dataset and some preprocessing is performed (i.e. remove usernames or weblinks).

A more detailed description of the pipeline can be found below.

To run the complete pipeline and automatically prepare data from the available sources, issue the following command:

```bash
bash prepare_data.sh
```

If you want to include your own data or include data from the extra sources below, you have to follow two steps:
1. Either (if available) integrate the download link in the script download_data.sh or move your data file to the data_raw directory.
2. Implement a function for preprocessing your data source in preprocess_data.py. This function should read your data file form the data_raw directory, convert to a dataframe with columns ["text", "label", "source"] whereby label takes value 0 for non-vulgar text and 1 for vulgar text and the column is a string for the name of your data source. This will be used later for splitting data into train, test and eval set. To allow crossvalidation, splitting is done right before the model training. Last, save the dataframe in the directory data_preprocessed as csv file.
Now you are set to run the complete pipeline, including your own data.


Next, we give a detailed overview of the data preparation pipeline.

##### <a name="raw">Raw data</a>
The raw data is stored in the `data/data_raw` directory. You can complement the download_data.sh script to include your own file. To download data, issue the following command:

```bash
bash download_data.sh
```

##### <a name="preprocess">Preprocessed data</a>
The preprocessed data is stored in the `data/data_preprocessed` directory. For every single data source there is a function in the file preprocess_data.py to transform it into a uniform format. This means the file has three columns: ['text', 'label', 'source'] whereby label==1 means vulgar text and label==0 stands for non-vulgar text. Source is an identifier for the data source. This columns will be used in splitting dataset into train, test and eval data and to make sure that all data sources are evenly represented in both train, test and eval.
Issue the following command to preprocess the data:

```bash
python preprocess_data.py
```

##### <a name="train">Training data</a>
The training data is stored in the `data/data_train` directory. All data sources are included and appended to one csv file.
To create the training data, issue the following command:

```bash
python create_train_data.py
```

##  Data sources overview

| Internal Name        | Description                                                                                                                                                                                                                                                                                                                                                         | Link                                                                                  | Count of examples after preparation | License                                                                   | Citation                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|-------------------------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | GermEval-2018        | This dataset comprises the training and test data (German tweets) from the GermEval 2018 Shared on Offensive Language Detection.                                                                                                                                                                                                                                    | https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML | 8407                                | Creative Commons Attribution 4.0 International License                    | Wiegand, Michael, 2019, "GermEval-2018 Corpus (DE)", https://doi.org/10.11588/data/0B5VML, heiDATA, V1                                                                                                                                                                                                                                                                                                                                                         |
| Germ Eval 2021       | We provide an annotated dataset of over 4,000 Facebook user comments that have been labeled by four trained annotators. The dataset is drawn from the Facebook page of a political talk show of a German television broadcaster, including user discussions from February till July 2019. The dataset is provided in anonymized form.                               | https://germeval2021toxic.github.io/SharedTask/                                       | 4188                                | Unknown                                                                   | Risch, Julian  and Stoll, Anke  and Wilms, Lena  and Wiegand, Michael, 2021, "Overview of the {G}erm{E}val 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments", Association for Computational Linguistics, pages 1-12                                                                                                                                                                                                       |
| RP-Mod               | RP-Mod & RP-Crowd: Moderator- and Crowd-Annotated German News Comment Datasets                                                                                                                                                                                                                                                                                      | https://zenodo.org/record/5291339#.YnluxS-206U                                        | 84998                               | Creative Commons Attribution Non Commercial Share Alike 4.0 International | Assenmacher, Dennis, Niemann, Marco, Müller, Kilian, Seiler, Moritz V., Riehle, Dennis M., & Trautmann, Heike. (2021). RP-Mod & RP-Crowd: Moderator- and Crowd-Annotated German News Comment Datasets (Version v2) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5291339                                                                                                                                                                                  |
| Tweets Refugees      | This dataset contains a German, annotated corpus of tweets regarding refugees in Germany. The tweets are annotated with hate speech ratings.                                                                                                                                                                                                                        | https://github.com/UCSM-DUE/IWG_hatespeech_public                                     | 369                                 | Creative Commons Attribution-ShareAlike 3.0 Unported License              | Björn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki, Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis, Proceedings of NLP4CMC III: 3rd Workshop on Natural Language Processing for Computer-Mediated Communication (Bochum) (Michael Beißwenger, Michael Wojatzki, and Torsten Zesch, eds.), Bochumer Linguistische Arbeitsberichte, vol. 17, sep 2016, pp. 6-9. |
| Facebook Hate Speech | We constructed three datasets by accessing publicly available Facebook pages. We crawled Facebook posts including the comments published in response to them from the Facebook pages “Pegida” (dataset 1), “Ich bin Patriot, aber kein Nazi” (“I’m a patriot, not a nazi”) (dataset 2) and “Kriminelle Ausländer raus” (“Criminal foreigners get out”) (dataset 3). | http://ub-web.de/research/  | 674                                 | Unknown                                                                   | Bretschneider, U.; Peters, R. (2017): Detecting Offensive Statements towards Foreigners in Social Media. In: Proceedings of the 50th Hawaii International Conference on System Sciences (HICSS), Hawaii, USA, January 4-7, 2017.                                                                                                                                                                                                                               |
 
### The following files are not publicly available and can be obtained by contacting the repository owner in the corresponding project:
| Internal Name    | Description                                                                                                                                                                                                                                                                                                                                                         | Link                                                                                  | Count of examples after preparation | License                                                                   | Citation                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Covid            | German Abusive Language Dataset with Focus on COVID-19                                                                                                                                                                                                                                                                                                              | https://github.com/mawic/german-abusive-language-covid-19                             | 4960 | Unknown                                                                   | Wich, Maximilian, Svenja Räther, and Georg Groh. "German Abusive Language Dataset with Focus on COVID-19." Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021). 2021.                                                                                                                                                                                                                                                             |
 | Telegram         | Introducing an Abusive Language Classification Framework for Telegram to Investigate the German Hater Community                                                                                                                                                                                                                                                     | https://arxiv.org/abs/2109.07346                                                      | 1099 | Unknown                                                                   | Wich, Maximilian, et al. "Introducing an abusive language classification framework for telegram to investigate the german hater community." Proceedings of the International AAAI Conference on Web and Social Media. Vol. 16. 2022.                                                                                                                                                                                                                           |
 

### Further sources which do not allow inclusion in this project by the license:
| Name    | Link                                                   | 
|------------------|--------------------------------------------------------|
| Hasoc | https://hasocfire.github.io/hasoc/2020/proceeding.html | 
| M-Phasis | https://github.com/uds-lsv/mphasis |
